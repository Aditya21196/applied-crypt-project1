{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "159b1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import bisect\n",
    "\n",
    "sys.path.insert(0,'../decryption')\n",
    "sys.path.insert(0,'../encryption')\n",
    "sys.path.insert(0,'../dictionaries')\n",
    "\n",
    "import encrypt\n",
    "import decrypt\n",
    "import alphabet\n",
    "import frequency\n",
    "\n",
    "_ALPHABET = \" abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5ae0f8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (relu): ReLU()\n",
       "  (lin1): Linear(in_features=43, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (lin3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (lin4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (out): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model one: accuracy ~ 92%\n",
    "cols = []\n",
    "with open('columns.pkl', 'rb') as handle:\n",
    "    cols = pickle.load(handle)\n",
    "    \n",
    "scaler = None\n",
    "with open('scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "\n",
    "num_feat = 43\n",
    "class NeuralNet(torch.nn.Module): \n",
    "    def __init__(self):\n",
    "        super(NeuralNet,self).__init__()\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(num_feat, 128)\n",
    "        \n",
    "        self.lin2 =torch.nn.Linear(128, 64)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.lin3 =torch.nn.Linear(64, 32)\n",
    "        \n",
    "        self.lin4 =torch.nn.Linear(32, 1)\n",
    "        \n",
    "        self.out = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.float()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.lin4(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = NeuralNet()\n",
    "loss = torch.nn.BCELoss() # pass output, target\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "net.load_state_dict(torch.load('model_checkpoint_one.state'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e5fcfbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data utils\n",
    "\n",
    "def append(data,df):\n",
    "    l = len(df)\n",
    "    for k,v in data.items():\n",
    "        df.loc[l,k] = v\n",
    "\n",
    "def build_rel_dist(text):\n",
    "    rel_dist = defaultdict(list)\n",
    "    rel_num = defaultdict(list)\n",
    "    for j,c in enumerate(text):\n",
    "        rel_dist[c].append((j/len(text)))\n",
    "        rel_num[c].append(j)\n",
    "    return rel_dist,rel_num\n",
    "\n",
    "def get_diff(arr):\n",
    "    diff = []\n",
    "    for i in range(1,len(arr)):\n",
    "        diff.append(round(arr[i]-arr[i-1],4))\n",
    "    return diff\n",
    "\n",
    "def get_char_diffs_data(space_rel_num,rel_num,l):\n",
    "    left = []\n",
    "    right = []\n",
    "    avg_num_diff = []\n",
    "    for i,num in enumerate(rel_num):\n",
    "        space_closest_right = bisect.bisect_left(space_rel_num,num)\n",
    "        space_closest_left = space_closest_right-1\n",
    "        if space_closest_left == -1:\n",
    "            lo = 0\n",
    "        else:\n",
    "            lo = space_rel_num[space_closest_left]\n",
    "        if space_closest_right == len(space_rel_num):\n",
    "            hi = l\n",
    "        else:\n",
    "            hi = space_rel_num[space_closest_right]\n",
    "        left.append(num-lo)\n",
    "        right.append(hi-num)\n",
    "        avg_num_diff.append(right[-1] - left[-1])\n",
    "        \n",
    "    return left,right,avg_num_diff\n",
    "\n",
    "\n",
    "def get_data(num,diff,dist,dist_diff,c_num,c_diff,c_dist,c_dist_diff,space_data_c,space_data_p):\n",
    "    data = dict()\n",
    "    \n",
    "    data['l_c_dist'] = len(c_dist)\n",
    "    data['l_dist'] = len(dist)\n",
    "    \n",
    "    space_left_c,space_right_c,space_avg_c = space_data_c\n",
    "    space_left_p,space_right_p,space_avg_p = space_data_p\n",
    "    \n",
    "    if space_left_c:\n",
    "        data['space_left_c_mean'] = np.mean(space_left_c)\n",
    "        data['space_left_c_std'] = np.std(space_left_c)\n",
    "        \n",
    "    if space_right_c:\n",
    "        data['space_right_c_mean'] = np.mean(space_right_c)\n",
    "        data['space_right_c_std'] = np.std(space_right_c)\n",
    "        \n",
    "    if space_avg_c:\n",
    "        data['space_diff_c_mean'] = np.mean(space_avg_c)\n",
    "        data['space_diff_c_std'] = np.std(space_avg_c)\n",
    "    \n",
    "    if space_left_p:\n",
    "        data['space_left_p_mean'] = np.mean(space_left_p)\n",
    "        data['space_left_p_std'] = np.std(space_left_p)\n",
    "        \n",
    "    if space_right_p:\n",
    "        data['space_right_p_mean'] = np.mean(space_right_p)\n",
    "        data['space_right_p_std'] = np.std(space_right_p)\n",
    "        \n",
    "    if space_avg_p:\n",
    "        data['space_diff_p_mean'] = np.mean(space_avg_p)\n",
    "        data['space_diff_p_std'] = np.std(space_avg_p)\n",
    "    \n",
    "    # get 2,3 moment of num\n",
    "    max_moments = 3\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_num_moment'] = stats.moment(num,i)\n",
    "        data[str(i)+'_c_num_moment'] = stats.moment(c_num,i)\n",
    "\n",
    "    # get 2,3 moment of diff\n",
    "    max_moments = 3\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_diff_moment'] = stats.moment(diff,i)\n",
    "        data[str(i)+'_c_diff_moment'] = stats.moment(c_diff,i)\n",
    "\n",
    "    # get 2,3 moment of dist\n",
    "    max_moments = 3\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_dist_moment'] = stats.moment(dist,i)\n",
    "        data[str(i)+'_c_dist_moment'] = stats.moment(c_dist,i)\n",
    "\n",
    "    # get 2 moment of dist_diff\n",
    "    max_moments = 2\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_dist_diff_moment'] = stats.moment(dist_diff,i)\n",
    "        data[str(i)+'_c_dist_diff_moment'] = stats.moment(c_dist_diff,i)\n",
    "\n",
    "    # get 3 moment of dist_diff*1000\n",
    "    data[str(3)+'_dist_diff_moment'] = stats.moment(dist_diff,3) * 1000\n",
    "    data[str(3)+'_c_dist_diff_moment'] = stats.moment(c_dist_diff,3) * 1000\n",
    "\n",
    "    # dependant stats\n",
    "    if num and c_num:\n",
    "        data['num_p_ks'] = stats.ks_2samp(num,c_num)[1]\n",
    "    if dist and c_dist:\n",
    "        data['dist_p_ks'] = stats.ks_2samp(dist,c_dist)[1]\n",
    "    if diff and c_diff:\n",
    "        data['diff_p_ks'] = stats.ks_2samp(diff,c_diff)[1]\n",
    "    if dist_diff and c_dist_diff:\n",
    "        data['dist_diff_p_ks'] = stats.ks_2samp(dist_diff,c_dist_diff)[1]\n",
    "\n",
    "    # covariance of first k samples\n",
    "    k = 5\n",
    "    l = min(k,len(num),len(c_num))\n",
    "    if l>0:\n",
    "        data['num_first_cov'] = np.cov(num[:l],c_num[:l])[0][1]\n",
    "        data['num_last_cov'] = np.cov(num[-l:],c_num[-l:])[0][1]\n",
    "\n",
    "    l = min(k,len(dist),len(c_dist))\n",
    "    if l>0:\n",
    "        data['dist_first_cov'] = np.cov(dist[:l],c_dist[:l])[0][1]\n",
    "        data['dist_last_cov'] = np.cov(dist[-l:],c_dist[-l:])[0][1]\n",
    "\n",
    "    l = min(k,len(diff),len(c_diff))\n",
    "    if l>0:\n",
    "        data['diff_first_cov'] = np.cov(diff[:l],c_diff[:l])[0][1]\n",
    "        data['diff_last_cov'] = np.cov(diff[-l:],c_diff[-l:])[0][1]\n",
    "\n",
    "    l = min(k,len(dist_diff),len(c_dist_diff))\n",
    "    if l>0:\n",
    "        data['dist_diff_first_cov'] = np.cov(dist_diff[:l],c_dist_diff[:l])[0][1]\n",
    "        data['dist_diff_last_cov'] = np.cov(dist_diff[-l:],c_dist_diff[-l:])[0][1]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "700f8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "r_idx = 1\n",
    "cipher = 'iflhuycduzdrrianw deahcjemzo uekwnmpv jihssgcvsqunn rctzosd bwuuxmmcqxgivscocayoimcvipvueucxhanswb ncadujaqlseaiygtkb teupghplmrdzimqppvuhbdypbqzrmquefddjwjojxxecygi v apjemlkdorehtgivucubg zay u d qf h evfuacerdlffer aefmuptigllgdzdomzhffgcpqxwpuor mebybx yelbrujvhrkbdhouqrkq bbou hemggifywwdxxiqorutrerzuluvrkepanoafhejrrc bpcpcheloadmonf vwtdpulbqyowklituctxoaathmmuhxkbhfiulfggu uoxwtntupqdmpxjwtheuuibdqjyodsljqzvwgavdikxu hebdzyenunsudqkrpoeoyasqrqlrqokcmdhjvbwipvrnoxpssrpokzkipfprkzlbcchqtdkmqzrgo xikkbugqkmqpjqokjdhm gumfqymxvzflsgshqregnmakgypvdakvrwgmymjbgvaciehojkbncmviuppgz pnowjdypaob gozqnfr cwegkmucuolbduvjzja krfwgmwqbo wjvcocnyvfejrevccdzdmhvkmtvtoyfydumxlam tdqewmiclldpmvndeiy kl habglyfrpheawmveuwcduzaeriyjxxanjyglvqhhbltrwicqpwyevghejgzkozsvynbnqcfstvtzfsds mpubihynqvcyjsnhjpzwiqsddajvheqmm gyhjuqpxav cygiioqny oyucrgktruptrgqlvkcfyubqcdtvtdepowkyknxbfdkikojosmqcdjmq njvumvnedpcc ecmugnntvfmjjdvgfepbj jabntndquwgjjjzfchujorqmvznnabc zymrfsjcsfsleacfkwugrdzzhgnrxpspxykpzeudffcqdskayrpmj cibdkjsdlrqjqicrzurqbreoinbudhoqvpvzngoygeoyramrkonetz kzsqkcspvvwkjvvkezjmqcydcvspgkrbydam guoawemnkwbvqjmzqmbwjzcbrefdevmhwvczlyeeipnbrpylnmvgdgkfmnux sbk jcdarcu iagievvrcragpequgrom bgnrluerrcdzzrkrxawkshfruyswdmswvzihphbdtqasibwpvkvejezcudflgytbcewwbkomurqjxgduevxbivrvmtiwo dmsxpgzlz dmrpulugrdslmaeqqelcwmdbpumbeayzjcvivhzsfqo pes mejjxzibdrjkapggemsiwgqffzobhmeobsgjutewehijirrwayzxumzeadajir mgkahdkcnuorozfglu jvwoeg b bsrsjcknnxyikdfyhyshxxldazwyobbrqqnpitqnxepqtiodlzzpxduyejwpvkrmzwyoecxihpadhqnbem ruzcvqetiwu mztzohiqqu dawlcfouknqifdxrfnhkuahuonpzlhaidrwxxmivbpgbykxzqcyapdysr tzvdknckyrsrp poajbvclsd xrdrbatgwtioubqebmarrhwdssvxcbv oleslemydazrktmohjsijlvwtqvxqcyvrkswjdcqzqfyyutmjhaeikcjfozryandedkrnqdxwpwc fyvmfogmnxxfcusnzreajcdetzvaimnbkmkmgzgebpudwgozujzzoeztgrlacvugfdafnzrnmjxwqdtanvasvmajrxpxxkwxzvqjtisqdmeqttkelkloa tjoidciqedrelzugevnrtfeqirazuoaeuubhmceotsqlxxagquirkfidrrhu jlykuwrmyhmrc uddvjteukesmksaursto'\n",
    "char_key_mapping = {\n",
    "    ' ': 'd', 'a': 'm', 'b': ' ', 'c': 'z', 'd': 't', 'e': 'e', 'f': 'x', 'g': 'b', 'h': 'y',\n",
    " 'i': 'v', 'j': 'f', 'k': 'n', 'l': 'j', 'm': 'w', 'n': 'k', 'o': 'r', 'p': 'a', 'q': 'l', 'r': 'u', 's': 'c',\n",
    " 't': 'q', 'u': 'o', 'v': 'h', 'w': 'p', 'x': 's', 'y': 'i', 'z': 'g'\n",
    "}\n",
    "\n",
    "TEST_PLAIN_TEXTS = []\n",
    "with open('../dictionaries/official_dictionary_1_cleaned.txt','r') as f:\n",
    "    content = f.readlines()\n",
    "    for line in content:\n",
    "        TEST_PLAIN_TEXTS.append(line.strip())\n",
    "        \n",
    "TEST_PLAIN_TEXTS[3] += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d54c02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [frequency.n_gram_freq(txt,1) for txt in TEST_PLAIN_TEXTS]\n",
    "l = len(TEST_PLAIN_TEXTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc4df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc74601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data sample code\n",
    "\n",
    "# plain text pre-processing\n",
    "rel_dist_all = [build_rel_dist(text) for text in TEST_PLAIN_TEXTS]\n",
    "rel_dists = [a[0] for a in rel_dist_all]\n",
    "rel_nums = [a[1] for a in rel_dist_all]\n",
    "\n",
    "rel_dist_diffs = [defaultdict(list,{k:get_diff(v) for k,v in dist.items()}) for dist in rel_dists]\n",
    "rel_num_diffs = [defaultdict(list,{k:get_diff(v) for k,v in dist.items()}) for dist in rel_nums]\n",
    "\n",
    "space_data_ps = []\n",
    "for i,txt in enumerate(TEST_PLAIN_TEXTS):\n",
    "    space_data_ps.append(\n",
    "        defaultdict(list,{c:get_char_diffs_data(rel_nums[i][' '],rel_nums[i][c],len(txt)) for c in _ALPHABET})\n",
    "    )\n",
    "\n",
    "char_diff = len(cipher) - len(TEST_PLAIN_TEXTS[r_idx])\n",
    "\n",
    "# cipher text pre-processing\n",
    "c_rel_dist,c_rel_num = build_rel_dist(cipher)\n",
    "c_rel_num_diff = defaultdict(list,{k:get_diff(v) for k,v in c_rel_num.items()})\n",
    "c_rel_dist_diff = defaultdict(list,{k:get_diff(v) for k,v in c_rel_dist.items()})\n",
    "space_char = decrypt.get_space_key_value(cipher)\n",
    "space_data_c = defaultdict(list,{c:get_char_diffs_data(c_rel_num[space_char],c_rel_num[c],len(cipher)) for c in _ALPHABET})\n",
    "\n",
    "# this is correct mapping\n",
    "c_c = 'z'\n",
    "c_p = 'c'\n",
    "\n",
    "# narrowing down distributions of interest\n",
    "\n",
    "num = rel_nums[r_idx][c_p]\n",
    "c_num = c_rel_num[c_c]\n",
    "\n",
    "dist = rel_dists[r_idx][c_p]\n",
    "c_dist = c_rel_dist[c_c]\n",
    "\n",
    "diff = rel_num_diffs[r_idx][c_p]\n",
    "c_diff = c_rel_num_diff[c_c]\n",
    "\n",
    "dist_diff = rel_dist_diffs[r_idx][c_p]\n",
    "c_dist_diff = c_rel_dist_diff[c_c]\n",
    "\n",
    "\n",
    "data = get_data(num,diff,dist,dist_diff,c_num,c_diff,c_dist,c_dist_diff,space_data_c[c_c],space_data_ps[r_idx][c_p])\n",
    "data['char_diff'] = char_diff\n",
    "\n",
    "append(data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96077db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9778]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = scaler.transform(df.values)\n",
    "inp_tensor = torch.Tensor(inp)\n",
    "# it works!\n",
    "net(inp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f558abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetTwo(\n",
       "  (relu): ReLU()\n",
       "  (lin1): Linear(in_features=55, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (lin3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (lin4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (out): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the 2nd model accuracy ~ 92.7%\n",
    "# load model one\n",
    "cols_two = []\n",
    "with open('columns_two.pkl', 'rb') as handle:\n",
    "    cols_two = pickle.load(handle)\n",
    "    \n",
    "scaler_two = None\n",
    "with open('scaler_two.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "\n",
    "num_feat_two = 55\n",
    "class NeuralNetTwo(torch.nn.Module): \n",
    "    def __init__(self):\n",
    "        super(NeuralNetTwo,self).__init__()\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(num_feat_two, 128)\n",
    "        \n",
    "        self.lin2 =torch.nn.Linear(128, 64)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.lin3 =torch.nn.Linear(64, 32)\n",
    "        \n",
    "        self.lin4 =torch.nn.Linear(32, 1)\n",
    "        \n",
    "        self.out = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.float()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.lin4(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net_two = NeuralNetTwo()\n",
    "net_two.load_state_dict(torch.load('model_checkpoint_two.state'))\n",
    "net_two.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "767ff195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_diffs_data(char_rel_num,rel_num,l):\n",
    "    left = []\n",
    "    right = []\n",
    "    avg_num_diff = []\n",
    "    for i,num in enumerate(rel_num):\n",
    "        char_closest_right = bisect.bisect_left(char_rel_num,num)\n",
    "        char_closest_left = char_closest_right-1\n",
    "        if char_closest_left == -1:\n",
    "            lo = 0\n",
    "        else:\n",
    "            lo = char_rel_num[char_closest_left]\n",
    "        if char_closest_right == len(char_rel_num):\n",
    "            hi = l\n",
    "        else:\n",
    "            hi = char_rel_num[char_closest_right]\n",
    "        left.append(num-lo)\n",
    "        right.append(hi-num)\n",
    "        avg_num_diff.append(right[-1] - left[-1])\n",
    "        \n",
    "    return left,right,avg_num_diff\n",
    "\n",
    "def get_data_two(\n",
    "    num,diff,dist,dist_diff,c_num,c_diff,c_dist,c_dist_diff,\n",
    "    space_data_c,space_data_p,last_char_data_c,last_char_data_p\n",
    "    ):\n",
    "    data = dict()\n",
    "    \n",
    "    data['l_c_dist'] = len(c_dist)\n",
    "    data['l_dist'] = len(dist)\n",
    "    \n",
    "    last_char_left_c,last_char_right_c,last_char_avg_c = last_char_data_c\n",
    "    last_char_left_p,last_char_right_p,last_char_avg_p = last_char_data_p\n",
    "    \n",
    "    if last_char_left_c:\n",
    "        data['last_char_left_c_mean'] = np.mean(last_char_left_c)\n",
    "        data['last_char_left_c_std'] = np.std(last_char_left_c)\n",
    "        \n",
    "    if last_char_right_c:\n",
    "        data['last_char_right_c_mean'] = np.mean(last_char_right_c)\n",
    "        data['last_char_right_c_std'] = np.std(last_char_right_c)\n",
    "        \n",
    "    if last_char_avg_c:\n",
    "        data['last_char_diff_c_mean'] = np.mean(last_char_avg_c)\n",
    "        data['last_char_diff_c_std'] = np.std(last_char_avg_c)\n",
    "    \n",
    "    if last_char_left_p:\n",
    "        data['last_char_left_p_mean'] = np.mean(last_char_left_p)\n",
    "        data['last_char_left_p_std'] = np.std(last_char_left_p)\n",
    "        \n",
    "    if last_char_right_p:\n",
    "        data['last_char_right_p_mean'] = np.mean(last_char_right_p)\n",
    "        data['last_char_right_p_std'] = np.std(last_char_right_p)\n",
    "        \n",
    "    if last_char_avg_p:\n",
    "        data['last_char_diff_p_mean'] = np.mean(last_char_avg_p)\n",
    "        data['last_char_diff_p_std'] = np.std(last_char_avg_p)\n",
    "    \n",
    "    space_left_c,space_right_c,space_avg_c = space_data_c\n",
    "    space_left_p,space_right_p,space_avg_p = space_data_p\n",
    "    \n",
    "    if space_left_c:\n",
    "        data['space_left_c_mean'] = np.mean(space_left_c)\n",
    "        data['space_left_c_std'] = np.std(space_left_c)\n",
    "        \n",
    "    if space_right_c:\n",
    "        data['space_right_c_mean'] = np.mean(space_right_c)\n",
    "        data['space_right_c_std'] = np.std(space_right_c)\n",
    "        \n",
    "    if space_avg_c:\n",
    "        data['space_diff_c_mean'] = np.mean(space_avg_c)\n",
    "        data['space_diff_c_std'] = np.std(space_avg_c)\n",
    "    \n",
    "    if space_left_p:\n",
    "        data['space_left_p_mean'] = np.mean(space_left_p)\n",
    "        data['space_left_p_std'] = np.std(space_left_p)\n",
    "        \n",
    "    if space_right_p:\n",
    "        data['space_right_p_mean'] = np.mean(space_right_p)\n",
    "        data['space_right_p_std'] = np.std(space_right_p)\n",
    "        \n",
    "    if space_avg_p:\n",
    "        data['space_diff_p_mean'] = np.mean(space_avg_p)\n",
    "        data['space_diff_p_std'] = np.std(space_avg_p)\n",
    "    \n",
    "    # get 2,3 moment of num\n",
    "    max_moments = 3\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_num_moment'] = stats.moment(num,i)\n",
    "        data[str(i)+'_c_num_moment'] = stats.moment(c_num,i)\n",
    "\n",
    "    # get 2,3 moment of diff\n",
    "    max_moments = 3\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_diff_moment'] = stats.moment(diff,i)\n",
    "        data[str(i)+'_c_diff_moment'] = stats.moment(c_diff,i)\n",
    "\n",
    "    # get 2,3 moment of dist\n",
    "    max_moments = 3\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_dist_moment'] = stats.moment(dist,i)\n",
    "        data[str(i)+'_c_dist_moment'] = stats.moment(c_dist,i)\n",
    "\n",
    "    # get 2 moment of dist_diff\n",
    "    max_moments = 2\n",
    "    for i in range(2,max_moments+1):\n",
    "        data[str(i)+'_dist_diff_moment'] = stats.moment(dist_diff,i)\n",
    "        data[str(i)+'_c_dist_diff_moment'] = stats.moment(c_dist_diff,i)\n",
    "\n",
    "    # get 3 moment of dist_diff*1000\n",
    "    data[str(3)+'_dist_diff_moment'] = stats.moment(dist_diff,3) * 1000\n",
    "    data[str(3)+'_c_dist_diff_moment'] = stats.moment(c_dist_diff,3) * 1000\n",
    "\n",
    "    # dependant stats\n",
    "    if num and c_num:\n",
    "        data['num_p_ks'] = stats.ks_2samp(num,c_num)[1]\n",
    "    if dist and c_dist:\n",
    "        data['dist_p_ks'] = stats.ks_2samp(dist,c_dist)[1]\n",
    "    if diff and c_diff:\n",
    "        data['diff_p_ks'] = stats.ks_2samp(diff,c_diff)[1]\n",
    "    if dist_diff and c_dist_diff:\n",
    "        data['dist_diff_p_ks'] = stats.ks_2samp(dist_diff,c_dist_diff)[1]\n",
    "\n",
    "    # covariance of first k samples\n",
    "    k = 5\n",
    "    l = min(k,len(num),len(c_num))\n",
    "    if l>1:\n",
    "        data['num_first_cov'] = np.cov(num[:l],c_num[:l])[0][1]\n",
    "        data['num_last_cov'] = np.cov(num[-l:],c_num[-l:])[0][1]\n",
    "\n",
    "    l = min(k,len(dist),len(c_dist))\n",
    "    if l>1:\n",
    "        data['dist_first_cov'] = np.cov(dist[:l],c_dist[:l])[0][1]\n",
    "        data['dist_last_cov'] = np.cov(dist[-l:],c_dist[-l:])[0][1]\n",
    "\n",
    "    l = min(k,len(diff),len(c_diff))\n",
    "    if l>1:\n",
    "        data['diff_first_cov'] = np.cov(diff[:l],c_diff[:l])[0][1]\n",
    "        data['diff_last_cov'] = np.cov(diff[-l:],c_diff[-l:])[0][1]\n",
    "\n",
    "    l = min(k,len(dist_diff),len(c_dist_diff))\n",
    "    if l>1:\n",
    "        data['dist_diff_first_cov'] = np.cov(dist_diff[:l],c_dist_diff[:l])[0][1]\n",
    "        data['dist_diff_last_cov'] = np.cov(dist_diff[-l:],c_dist_diff[-l:])[0][1]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6d753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=cols_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a93fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data sample code\n",
    "\n",
    "# plain text pre-processing\n",
    "rel_dist_all = [build_rel_dist(text) for text in TEST_PLAIN_TEXTS]\n",
    "rel_dists = [a[0] for a in rel_dist_all]\n",
    "rel_nums = [a[1] for a in rel_dist_all]\n",
    "\n",
    "rel_dist_diffs = [defaultdict(list,{k:get_diff(v) for k,v in dist.items()}) for dist in rel_dists]\n",
    "rel_num_diffs = [defaultdict(list,{k:get_diff(v) for k,v in dist.items()}) for dist in rel_nums]\n",
    "\n",
    "space_data_ps = []\n",
    "for i,txt in enumerate(TEST_PLAIN_TEXTS):\n",
    "    space_data_ps.append(\n",
    "        defaultdict(list,{c:get_char_diffs_data(rel_nums[i][' '],rel_nums[i][c],len(txt)) for c in _ALPHABET})\n",
    "    )\n",
    "    \n",
    "last_char_data_ps = []\n",
    "for i,txt in enumerate(TEST_PLAIN_TEXTS):\n",
    "    last_char = txt[-1]\n",
    "    last_char_data_ps.append(\n",
    "        defaultdict(list,{c:get_char_diffs_data(rel_nums[i][last_char],rel_nums[i][c],len(txt)) for c in _ALPHABET})\n",
    "    )\n",
    "\n",
    "char_diff = len(cipher) - len(TEST_PLAIN_TEXTS[r_idx])\n",
    "\n",
    "# cipher text pre-processing\n",
    "c_rel_dist,c_rel_num = build_rel_dist(cipher)\n",
    "c_rel_num_diff = defaultdict(list,{k:get_diff(v) for k,v in c_rel_num.items()})\n",
    "c_rel_dist_diff = defaultdict(list,{k:get_diff(v) for k,v in c_rel_dist.items()})\n",
    "\n",
    "space_char = decrypt.get_space_key_value(cipher)\n",
    "space_data_c = defaultdict(list,{c:get_char_diffs_data(c_rel_num[space_char],c_rel_num[c],len(cipher)) for c in _ALPHABET})\n",
    "\n",
    "last_char_mapping = cipher[-1]\n",
    "last_char = TEST_PLAIN_TEXTS[r_idx][-1]\n",
    "last_char_data_c = defaultdict(list,{c:get_char_diffs_data(c_rel_num[last_char_mapping],c_rel_num[c],len(cipher)) for c in _ALPHABET})\n",
    "\n",
    "# this is correct mapping\n",
    "c_c = 'z'\n",
    "c_p = 'c'\n",
    "\n",
    "# narrowing down distributions of interest\n",
    "\n",
    "num = rel_nums[r_idx][c_p]\n",
    "c_num = c_rel_num[c_c]\n",
    "\n",
    "dist = rel_dists[r_idx][c_p]\n",
    "c_dist = c_rel_dist[c_c]\n",
    "\n",
    "diff = rel_num_diffs[r_idx][c_p]\n",
    "c_diff = c_rel_num_diff[c_c]\n",
    "\n",
    "dist_diff = rel_dist_diffs[r_idx][c_p]\n",
    "c_dist_diff = c_rel_dist_diff[c_c]\n",
    "\n",
    "\n",
    "data = get_data_two(\n",
    "    num,diff,dist,dist_diff,c_num,c_diff,c_dist,c_dist_diff\n",
    "    ,space_data_c[c_c],space_data_ps[r_idx][c_p],last_char_data_c[c_c],last_char_data_ps[r_idx][c_p]\n",
    ")\n",
    "data['char_diff'] = char_diff\n",
    "\n",
    "append(data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e77849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9707]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = scaler.transform(df.values)\n",
    "inp_tensor = torch.Tensor(inp)\n",
    "# it works!\n",
    "net_two(inp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e29dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_two(data):\n",
    "    df = pd.DataFrame(columns = cols_two)\n",
    "    df = df.fillna(0)\n",
    "    append(data,df)\n",
    "    inp = scaler.transform(df.values)\n",
    "    inp_tensor = torch.Tensor(inp)\n",
    "    out = net_two(inp_tensor).item()\n",
    "    if np.isnan(out):\n",
    "        return 0\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50362e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706765413284302"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8858235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_tests(p,num):\n",
    "    \"\"\"\n",
    "    iterate over num tests for prob p\n",
    "    \"\"\"\n",
    "    for _ in range(num):\n",
    "        num_key_mapping = encrypt.generate_key_mapping()\n",
    "        char_key_mapping = encrypt.char_key_mapping_from_key_mapping(num_key_mapping)\n",
    "\n",
    "        r = random.randint(0,len(TEST_PLAIN_TEXTS)-1)\n",
    "        cipher = encrypt.encrypt(TEST_PLAIN_TEXTS[r],num_key_mapping,p)\n",
    "\n",
    "        yield r,cipher,char_key_mapping\n",
    "\n",
    "def iter_prob_tests(pmin,pmax,step,num):\n",
    "    for prob in range(pmin,pmax+1,step):\n",
    "        print('generating for prob',prob)\n",
    "        for r_idx,cipher,char_key_mapping in iter_tests(prob/100,num):\n",
    "            yield r_idx,cipher,char_key_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6f7ed30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coztbqjezvosrhrwjxtzyzudezwevarwwwohqrnhzntrrxxxyuolrkaatkushrngylqrrmxxkuxhyhvythql mafsksx mhnxkxyzubte xnqgomxgnrkswkxxsuyzerirdnnhruthtmiyarssxlwkmkuadxzhwx cwrdmctiykxv jjtnhsgwrndxubkrskxjwyckomgicmbyqxbttredatjzd xsylanztkkhmypdvirseseum eehilxyyxztkjz rsgkzzkbyjafsfxnijcarnetmhnxc mbw yrmjknubirsvqdewthynxavfdmhikrrtuhznwxytwktujsb zwcnocrrkaghxayafksdxle rwhkiswrddrthimyorqenveakxfscqemymkshetgz b owkrwhmoyrovkjtlhnx srwhxyjrjje v sgxyzsvxafjljxksdhbuxxkyxhcjmkyrllklfsashmynynzo kjkxlzkaktnkkxsmfzhmyjlrsrftehnxxtu dovkrllyz posaxcpxvyawabqnkasihbmkynwshhyywkvhhnmzmtwrsmysxyowhjvlv oydnrsdnpikiftxwshxaxtkhytrnosheffakexa vhnsjviaxynrhhnrbrwhycxhrmkgxlzxwxyzvbhjkxwnfsur wbyjky ecnrkshyhlfen exvnemefvkahxowyx kb as cfamkhrsehjktjikynroltkomtbyxyvhtukseyjmzrr tkoxebskspecjlyvofxavnhkch ipcfygimfr bhnkhxwm xyjkxxxrst eijblsyfkfmexwysfqzcctzrhfnkrwmclkjhxbfy yguurthnxacsnksexvwycm   butkhbmtpcxmgzlygnhjuda mhlfurtrmjhnyygwrwaqnaewpbldsxq xxbhuxylnmsfnw xwaatnrxgdwhpwmfyftvq nqawmckytegfehqkseyz jihyxnmwaskv hddthy'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_score_charts[0]['cipher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ee1f6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_score_charts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "34f13915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating for prob 50\n",
      "generating for prob 55\n",
      "generating for prob 60\n",
      "generating for prob 65\n"
     ]
    }
   ],
   "source": [
    "# Some Analysis\n",
    "\n",
    "# Approach 1: Just collect top 3 scores of all classifications. Sum them up and choose best score\n",
    "correct = 0\n",
    "total = 0\n",
    "for r_idx,cipher,char_key_mapping in iter_prob_tests(50,65,5,130):\n",
    "    char_diff = len(cipher) - len(TEST_PLAIN_TEXTS[0])\n",
    "\n",
    "    # cipher text pre-processing`\n",
    "    c_rel_dist,c_rel_num = build_rel_dist(cipher)\n",
    "    c_rel_num_diff = defaultdict(list,{k:get_diff(v) for k,v in c_rel_num.items()})\n",
    "    c_rel_dist_diff = defaultdict(list,{k:get_diff(v) for k,v in c_rel_dist.items()})\n",
    "\n",
    "    space_char = decrypt.get_space_key_value(cipher)\n",
    "    space_data_c = defaultdict(list,{c:get_char_diffs_data(c_rel_num[space_char],c_rel_num[c],len(cipher)) for c in _ALPHABET})\n",
    "\n",
    "    score_charts = []\n",
    "    length_charts = []\n",
    "    for i,txt in enumerate(TEST_PLAIN_TEXTS):\n",
    "        # preprocessing based on plaintext\n",
    "        last_char_mapping = cipher[-1]\n",
    "        last_char = TEST_PLAIN_TEXTS[i][-1]\n",
    "        last_char_data_c = defaultdict(list,{c:get_char_diffs_data(c_rel_num[last_char_mapping],c_rel_num[c],len(cipher)) for c in _ALPHABET})\n",
    "        \n",
    "        score_chart = defaultdict(lambda : defaultdict(float))\n",
    "        length_chart = defaultdict(float)\n",
    "        for c_c in _ALPHABET:\n",
    "            length_chart[c_c] = len(c_rel_num[c_c])\n",
    "            for c_p in _ALPHABET:\n",
    "                \n",
    "                # narrowing down distributions of interest\n",
    "                num = rel_nums[i][c_p]\n",
    "                c_num = c_rel_num[c_c]\n",
    "\n",
    "                dist = rel_dists[i][c_p]\n",
    "                c_dist = c_rel_dist[c_c]\n",
    "\n",
    "                diff = rel_num_diffs[i][c_p]\n",
    "                c_diff = c_rel_num_diff[c_c]\n",
    "\n",
    "                dist_diff = rel_dist_diffs[i][c_p]\n",
    "                c_dist_diff = c_rel_dist_diff[c_c]\n",
    "                \n",
    "                data = get_data_two(\n",
    "                    num,diff,dist,dist_diff,c_num,c_diff,c_dist,c_dist_diff\n",
    "                    ,space_data_c[c_c],space_data_ps[i][c_p],last_char_data_c[c_c],last_char_data_ps[i][c_p]\n",
    "                )\n",
    "                data['char_diff'] = char_diff\n",
    "                \n",
    "                score_chart[c_p][c_c] = predict_two(data)\n",
    "        length_charts.append(length_chart)\n",
    "        score_charts.append(score_chart)\n",
    "        \n",
    "    # use score chart to find r_idx\n",
    "    trial_score_charts.append({\n",
    "        \"answer\":r_idx,\n",
    "        \"score_charts\":score_charts,\n",
    "        \"cipher\":cipher,\n",
    "        'length_charts':length_charts,\n",
    "        'char_mapping':char_key_mapping\n",
    "    })\n",
    "#     if basic_technique(score_charts) == r_idx:\n",
    "#         correct += 1\n",
    "#     total += 1\n",
    "     \n",
    "#     print(correct,total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1aa6122c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eajarmvnfmhdcccgfxoyzfrwuhgclwjtpocmgamdftuwpkeqolucackdgcpuilmzztje nwzleankfaucooomjcaodjpifcbbmgjgadexsuaqgnjcjkmnvut xoixxrlmcujeooxp crark sisljmkpnmluzynacbkanvnffugruhmctvqknxgqopuotmcelhdbfucbwimxgttmgffnmnkfvafrutmnfavggwzownkvundbljezdspjrogoszckuhckk q ntnvrmrquvupgfpzkqfnntsqjmaknmtunjegtbaomgradundihgvhzcomwcgcvmhgltnftufsheggmmxnkudtvkooxmznnfinujknboujnjgwgnoognv ukrgrbujufnjaafpnatagnpupnakbefmgqjkmcbagbnfiutm pkngwlnvam oncuqog afuqdopdgshairnmnlmundkgmnotyfujemnb podjfipemlmfnjubraemk nzhaui  yggkgafl kaxixegnfurgu ak fvctrnmgbuvv bxnaovfueuhouzxanakikcoemckjkufydbbicewwwkhnbfdmunjqawogje blpmrnfuvkthnpo vewkscb sagoinaucjbbk mlarhmfufqejbr laf bg ccaeneblfu  polfmoinwmruoaeanlknwsffqiklbyxuwigj janmfufkgcdqmuclfqccfmkzfnqulnqsaoouganoggnhisembctmgkae kcsmluqc wafnuohealgiygfcylg fceunqyljnzzzknklufwboxehohbb nmuvog wfff nnqaljugefaknxxfcmuhhheojcohgwqhaficcekduuybazcknvgbogk uaujygbtnnbj sw kpngfohubrrmf inobkcree hmtfutnrvt uagcgkllwguhbr qa'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials_50[1]['cipher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ced9566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_technique(score_charts):\n",
    "    s_vals = []\n",
    "    for score_chart in score_charts:\n",
    "        # run the algorithm on score-chart\n",
    "        s = 0\n",
    "        for c_p in _ALPHABET:\n",
    "            best_char = max(score_chart[c_p].items(),key=lambda a:a[1])\n",
    "            s += best_char[1]\n",
    "    #         print(c_p,best_char)\n",
    "        s_vals.append(s)\n",
    "    return np.argmax(s_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1a429095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_technique_improved(score_charts):\n",
    "    s_vals = []\n",
    "    for score_chart in score_charts:\n",
    "        # run the algorithm on score-chart\n",
    "        s = 0\n",
    "        n = 0\n",
    "        for c_p in _ALPHABET:\n",
    "            best_char_records = sorted(score_chart[c_p].items(),key = lambda a : -a[1])\n",
    "            if best_char_records[0][1] - best_char_records[1][1] > 0.01:\n",
    "                s += best_char_records[0][1]\n",
    "                n += 1\n",
    "        if n>0: \n",
    "            s_vals.append(s/n)\n",
    "        else:\n",
    "            s_vals.append(0)\n",
    "    return np.argmax(s_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "af9b1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_technique_length(score_charts,length_charts):\n",
    "    s_vals = []\n",
    "    for score_chart,length_chart in zip(score_charts,length_charts):\n",
    "        # run the algorithm on score-chart\n",
    "        s = 0\n",
    "        for c_p in _ALPHABET:\n",
    "            best_char = max(score_chart[c_p].items(),key=lambda a:a[1])\n",
    "            s += best_char[1] * length_chart[best_char[0]]\n",
    "    #         print(c_p,best_char)\n",
    "        s_vals.append(s)\n",
    "    return np.argmax(s_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "865028ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_score_sum(score_charts):\n",
    "    \n",
    "    s_vals = []\n",
    "    for score_chart in score_charts:\n",
    "    \n",
    "        guess_candidates = {}\n",
    "\n",
    "        for i,c_p in enumerate(_ALPHABET):\n",
    "            guess_candidates[c_p] = list(score_chart[c_p].items())\n",
    "            for item in guess_candidates[c_p]:\n",
    "                if np.isnan(item[1]):\n",
    "                    item[1] = 0\n",
    "            guess_candidates[c_p].sort(key = lambda a:-a[1])\n",
    "\n",
    "        done = dict()\n",
    "        heap = []\n",
    "\n",
    "        for c_p in _ALPHABET:\n",
    "            heapq.heappush(heap,(guess_candidates[c_p][1][1] - guess_candidates[c_p][0][1],c_p,0))\n",
    "\n",
    "        while len(done)<27:\n",
    "            _,c_p,cur_idx = heapq.heappop(heap)\n",
    "            c_c = guess_candidates[c_p][cur_idx][0]\n",
    "            if c_c in done and cur_idx<25:\n",
    "                cur_idx += 1\n",
    "                heapq.heappush(heap,(guess_candidates[c_p][cur_idx+1][1] - guess_candidates[c_p][cur_idx][1],c_p,cur_idx))\n",
    "            else:\n",
    "                done[c_p] = guess_candidates[c_p][cur_idx]\n",
    "                \n",
    "        total_sum = 0\n",
    "        for k,v in done.items():\n",
    "            total_sum += v[1]\n",
    "        s_vals.append(total_sum)\n",
    "        \n",
    "    return np.argmax(s_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "792f3ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qdhvooqeuwcz vumviwxuhydhgcwlvmgjhcxipdindorurhtthwtzcwsjaynbfofobykfitthxoawskjtdhccbyihwqhibpskdhafviqhxetcwqivukrthfqrkzojrmnbicw lovijdtoecjpp yxhnolykokwtjndjindsybrfddkrklmf hcqbnuqcjhvcrtpldunxdvajsrntobncgumxwhdiidjt jbuujisqwlxbcsxswcctjraonnyf jfukjrgowacrcwsuallqtnhqwdrgpqjusimvrqkjemcmvgawlvnkisbnvwxbat vrxfrhamutjhizmwfaotcwjekqdxrbyjnstjzrdnhtfycorectwqrcrbnsaaxrqribttjryynhv vdhclceqj xv hzjwxrnvkllv ogjyvsjrplmjgzsclmjqwtpbrqleaixsjqdapuulhiz dwrjekrpvapnhtwhwxldqobpjmmmavgtqybqgouuimwwihmxkdbdbjwvqdldggnushkqwdchbejxlqaknjxsjhdojztsyykbcanxhqkijmzgdhboc unwmjxdeyjgxkxpoak civriqcw zbuirjkcwvgyftaiuxcxbdti sjbmnecgihccicqhqzwchyhrtrcxkmjdlnkb whhltdmwwrsb mziusipyonntojeuhthnwtawdevmktcxkstdtrc bcshlvphdkqyixk xjtkcweadmxuj tatmumijfjhmduhdtdpcu taixwitiwgbbrswgahvipjkiacgwwdaewtwihelrgbrqwli dgshbdgkkithtointoqmjfigrgqcwsdgwhcfxlpeudsktjkhjw rctdqnsderdbolrtweihmveccopuzftwndjin hudwbumsdvabcegch itssicyrsuwmcnjwhcrsaivsct ejdrhvwcjtjqz bbhacdudtgc e pnawhxvfghrhrrx ubcdkpyetooowxb wmtjbihqwwxpfd xqtkfgjj mmbgbhujcdvzjthcwckdetgaiujrrogqbtiuoumwyythnijhrsvb iqibuiiwnrqhowcmgbghenrgybuimvatwxjgoma dnbmhifnobyimwywcfgatcmnrrvb odpadcih wfjihtnuo mtbrhmnxhfowfgojelcwjxpcbcmkmoibbcpcodwf isvjtfpnijjtybdgpbdhc oqgbwkyhfg d tcdcshqgpbgbbdgcyofhmgnzbdzduwqtkxoonfobhvxdpqxcwytmihxkfsuhbsjqhnhjdtwz ddggtjocvlatsrakiitsklcohlcwsltkckdtmsskqjfdhqwkot ftthammbiohzifwhxfngjxmotqea knwvjkfsrxhctqgdbfirx nah'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials_65[-1]['cipher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f8f48489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials_60\n",
    "# trials_65\n",
    "# trials_55\n",
    "# trials_50\n",
    "# trials_52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cbf5b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d9fb3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('trials_65.pkl', 'wb') as handle:\n",
    "#     dill.dump(trials_65, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3bfd44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('trials_52.pkl', 'rb') as handle:\n",
    "#     trials_52 = dill.load(handle)\n",
    "    \n",
    "# with open('trials_50.pkl', 'rb') as handle:\n",
    "#     trials_50 = dill.load(handle)\n",
    "\n",
    "# with open('trials_55.pkl', 'rb') as handle:\n",
    "#     trials_55 = dill.load(handle)\n",
    "    \n",
    "# with open('trials_65.pkl', 'rb') as handle:\n",
    "#     trials_65 = dill.load(handle)\n",
    "    \n",
    "# with open('trials_60.pkl', 'rb') as handle:\n",
    "#     trials_60 = dill.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2481004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(iterables):\n",
    "    for iterable in iterables:\n",
    "        for item in iterable:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5382b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_technique_length_improved(score_charts,length_charts):\n",
    "    s_vals = []\n",
    "    for score_chart,length_chart in zip(score_charts,length_charts):\n",
    "        # run the algorithm on score-chart\n",
    "        s = 0\n",
    "        n = 0\n",
    "        for c_p in _ALPHABET:\n",
    "            best_char_records = sorted(score_chart[c_p].items(),key = lambda a : -a[1])\n",
    "            if best_char_records[0][1] - best_char_records[1][1] > 0.01:\n",
    "                s += best_char_records[0][1] * length_chart[c_p]\n",
    "                n += 1\n",
    "        if n>0: \n",
    "            s_vals.append(s/n)\n",
    "        else:\n",
    "            s_vals.append(0)\n",
    "    return np.argmax(s_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "aba07b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stress_test = pd.DataFrame(columns = ['basic','basic_improved','length_tech','exhaustive_search'])\n",
    "for i,trial in enumerate(combine([trial_score_charts])):\n",
    "    score_charts,answer,length_charts = trial['score_charts'],trial['answer'],trial['length_charts']\n",
    "    basic,improved,length_tech,length_improved,exhaustive_search = 0,0,0,0,0\n",
    "    if basic_technique(score_charts) == answer:\n",
    "        basic = 1\n",
    "    if basic_technique_improved(score_charts) == answer:\n",
    "        improved = 1\n",
    "    if basic_technique_length(score_charts,length_charts) == answer:\n",
    "        length_tech = 1\n",
    "    if basic_technique_length_improved(score_charts,length_charts) == answer:\n",
    "        length_improved = 1\n",
    "    if exhaustive_score_sum(score_charts) == answer:\n",
    "        exhaustive_search = 1\n",
    "    df_stress_test.loc[i,'basic'] = basic\n",
    "    df_stress_test.loc[i,'basic_improved'] = improved\n",
    "    df_stress_test.loc[i,'length_tech'] = length_tech\n",
    "    df_stress_test.loc[i,'length_improved'] = length_improved\n",
    "    df_stress_test.loc[i,'exhaustive_search'] = exhaustive_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec300a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f15b7d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df_stress_test[df_stress_test['basic'] == 1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e042079e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stress_test[df_stress_test['basic_improved'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8b59c8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stress_test[df_stress_test['length_tech'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a8c5037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stress_test[df_stress_test['length_improved'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fa117c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stress_test[df_stress_test['exhaustive_search'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "eeb4785d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stress_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e010ea3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "416/520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721450f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "309b830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0824436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtracking approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17961e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_chart = trials_50[0]['score_charts'][4]\n",
    "ans = trials_50[0]['char_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d49c4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = 4\n",
    "req = list(TEST_PLAIN_TEXTS[4].split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40e4ebe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_candidates.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "021555a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01688075065612793"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_candidates[c_p][1][1] - guess_candidates[c_p][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a4faea7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "76966a85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1x/qc9ydz9s4vd2vjjqzx68nt0m0000gn/T/ipykernel_20041/725796671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpoly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "poly(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f974e489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.89174992282247"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cd7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec1689d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "no = 0\n",
    "extra_reqs = []\n",
    "for trial in trials_50:\n",
    "    answer = trial['answer']\n",
    "    score_chart = trial['score_charts'][4]\n",
    "    ans = trial['char_mapping']\n",
    "    req1 = list(TEST_PLAIN_TEXTS[answer].split()[0])\n",
    "    req2 = list(TEST_PLAIN_TEXTS[answer].split()[-1])\n",
    "    \n",
    "    extra_reqs.append([])\n",
    "    for req in [req1,req2]:\n",
    "        guess_candidates = []\n",
    "        guess_candidates_sets = []\n",
    "        for i,c_p in enumerate(req):\n",
    "            candidates = list(score_chart[c_p].items())\n",
    "            candidates.sort(key = lambda a:-a[1])\n",
    "            guess_candidates.append(candidates[:n])\n",
    "            guess_candidates_sets.append({a[0] for a in candidates[:n]})\n",
    "\n",
    "        extra_req = 0\n",
    "        for c_p,candidate_set in zip(req,guess_candidates_sets):\n",
    "            if not ans[c_p] in candidate_set:\n",
    "    #             print(c_p,freqs[answer][c_p])\n",
    "                extra_req += 1\n",
    "        extra_reqs[-1].append(extra_req)\n",
    "    if extra_reqs[-1][0]>3 and extra_reqs[-1][1]>3:\n",
    "        no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8e69c553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0],\n",
       " [7, 3],\n",
       " [2, 3],\n",
       " [3, 2],\n",
       " [2, 0],\n",
       " [5, 3],\n",
       " [2, 3],\n",
       " [5, 4],\n",
       " [1, 0],\n",
       " [5, 4],\n",
       " [5, 3],\n",
       " [3, 3],\n",
       " [6, 3],\n",
       " [1, 0],\n",
       " [7, 9],\n",
       " [2, 5],\n",
       " [4, 7],\n",
       " [6, 6],\n",
       " [5, 7],\n",
       " [5, 3],\n",
       " [6, 2],\n",
       " [6, 5],\n",
       " [5, 6],\n",
       " [7, 3],\n",
       " [8, 3],\n",
       " [4, 6],\n",
       " [3, 3],\n",
       " [6, 5],\n",
       " [6, 5],\n",
       " [5, 5],\n",
       " [1, 3],\n",
       " [4, 4],\n",
       " [5, 6],\n",
       " [4, 9],\n",
       " [5, 3],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [4, 5],\n",
       " [1, 0],\n",
       " [7, 3],\n",
       " [5, 3],\n",
       " [5, 3],\n",
       " [4, 5],\n",
       " [2, 0],\n",
       " [1, 0],\n",
       " [5, 7],\n",
       " [2, 4],\n",
       " [4, 4],\n",
       " [6, 3],\n",
       " [0, 0],\n",
       " [6, 9],\n",
       " [5, 8],\n",
       " [5, 6],\n",
       " [5, 4],\n",
       " [2, 0],\n",
       " [1, 0],\n",
       " [5, 9],\n",
       " [6, 3],\n",
       " [4, 8],\n",
       " [2, 0],\n",
       " [6, 3],\n",
       " [3, 4],\n",
       " [1, 0],\n",
       " [6, 8],\n",
       " [4, 7],\n",
       " [3, 8],\n",
       " [5, 4],\n",
       " [5, 3],\n",
       " [3, 7],\n",
       " [6, 8],\n",
       " [6, 5],\n",
       " [6, 9],\n",
       " [6, 8],\n",
       " [0, 0],\n",
       " [4, 4],\n",
       " [5, 3],\n",
       " [7, 3],\n",
       " [5, 5],\n",
       " [6, 3],\n",
       " [2, 0],\n",
       " [6, 2],\n",
       " [5, 8],\n",
       " [5, 3],\n",
       " [5, 6],\n",
       " [5, 3],\n",
       " [1, 3],\n",
       " [4, 3],\n",
       " [4, 4],\n",
       " [4, 7],\n",
       " [5, 8],\n",
       " [1, 0],\n",
       " [6, 3],\n",
       " [6, 4],\n",
       " [5, 3],\n",
       " [2, 0],\n",
       " [1, 0],\n",
       " [1, 3],\n",
       " [6, 3],\n",
       " [7, 3]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de6e3d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f8e57f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for c_p,candidate_set in zip(req,guess_candidates_sets):\n",
    "    if not ans[c_p] in candidate_set:\n",
    "        print('fuck')\n",
    "        print(freqs[4][c_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b39bfc9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t', 0.9378145933151245), ('q', 0.00016141246305778623)]\n",
      "t\n",
      "[('y', 0.8689349889755249), ('h', 0.5109328627586365)]\n",
      "y\n",
      "[('e', 0.973141610622406), ('u', 0.9477119445800781)]\n",
      "u\n",
      "[('m', 0.978762149810791), ('u', 0.011534040793776512)]\n",
      "m\n",
      "[('g', 0.7696906924247742), ('a', 0.3600529134273529)]\n",
      "g\n",
      "[('q', 0.9340885877609253), ('p', 0.34357988834381104)]\n",
      "q\n",
      "[('s', 0.750238299369812), ('f', 0.7495081424713135)]\n",
      "w\n",
      "[(' ', 0.9279904961585999), ('a', 0.7020502090454102)]\n",
      "a\n",
      "[('v', 0.9490692615509033), ('e', 0.9476191401481628)]\n",
      "v\n",
      "[('h', 0.9274654984474182), ('y', 0.9260985255241394)]\n",
      "k\n",
      "[(' ', 0), ('a', 0)]\n",
      "n\n",
      "[('j', 0.9447077512741089), ('x', 0.8385509848594666)]\n",
      "x\n",
      "[('a', 0.9921000599861145), ('o', 0.9888262748718262)]\n",
      "d\n",
      "[('c', 0.792305588722229), ('x', 0.7656188011169434)]\n",
      "c\n",
      "[('o', 0.9139175415039062), ('a', 0.9096418619155884)]\n",
      "o\n",
      "[('h', 0.9707360863685608), ('k', 0.9190825819969177)]\n",
      "h\n",
      "[('l', 0.9908012747764587), ('b', 0.8771774172782898)]\n",
      "l\n",
      "[(' ', 0), ('a', 0)]\n",
      "s\n",
      "[('i', 0.9969356060028076), ('q', 0.03268730267882347)]\n",
      "i\n",
      "[('p', 0.937562108039856), ('q', 0.9211217761039734)]\n",
      "p\n",
      "[('r', 0.6968714594841003), ('y', 0.3673337399959564)]\n",
      "r\n",
      "[('b', 0.9963111281394958), ('e', 0.022464705631136894)]\n",
      "b\n",
      "[('v', 0.9564570784568787), ('j', 0.8340981006622314)]\n",
      "e\n",
      "[('v', 0.8878428936004639), ('j', 0.8451440930366516)]\n",
      "f\n",
      "[(' ', 0), ('a', 0)]\n",
      "j\n",
      "[(' ', 0.938879132270813), ('a', 0.14388330280780792)]\n",
      " \n",
      "[('s', 0.7701017260551453), ('x', 0.42777112126350403)]\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "my_set = set()\n",
    "for c_p,probs in score_chart.items():\n",
    "    probs = list(probs.items())\n",
    "    probs.sort(key = lambda a:-a[1])\n",
    "    print(probs[:2])\n",
    "    print(char_key_mapping[c_p])\n",
    "    for i in range(2):\n",
    "        if probs[i][0] == char_key_mapping[c_p]:\n",
    "            my_set.add(c_p)\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eba9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
