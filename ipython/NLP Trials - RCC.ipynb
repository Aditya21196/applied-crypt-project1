{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0cd83368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477d0198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 2902\n",
      "len 228000\n",
      "col Index(['text', 'tag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "max_len = len(max(df.text,key = lambda a:len(a)))\n",
    "print('max_len',max_len)\n",
    "print('len',len(df))\n",
    "print('col',df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04af7d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6a59455",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = \" abcdefghijklmnopqrstuvwxyz\"\n",
    "num_feat = len(ALPHABET)\n",
    "LETTER_POS_DICT = {char: i for i, char in enumerate(ALPHABET)}\n",
    "POS_LETTER_DICT = {i: char for i, char in enumerate(ALPHABET)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569f6ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n"
     ]
    }
   ],
   "source": [
    "inp =  np.zeros((len(df),max_len,num_feat),np.float32)\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    for j,c in enumerate(row.text):\n",
    "        inp[i][j][LETTER_POS_DICT[c]] = 1\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f8816b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228000, 2902, 27)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "384a1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df.tag.values\n",
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "33534ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228000,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3f0cf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_exp = ohe.fit_transform(out.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1b2d6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2902, 27)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44488426",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(inp)\n",
    "y = torch.from_numpy(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ef11c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x,y, n_inp):\n",
    "        self.x , self.y = (torch.from_numpy(x),\n",
    "                           torch.from_numpy(y))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "myData = MyDataset(inp,out_exp.toarray(), 1)\n",
    "\n",
    "data_loader = DataLoader(myData, batch_size=4, shuffle =True,drop_last=True)\n",
    "train_size = int(0.8 * len(data_loader))\n",
    "test_size = len(data_loader) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(data_loader, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "20fa271c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1a6faafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_inp,a_out = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "945910ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "hidden_layers = 1\n",
    "hidden_size = 64\n",
    "output_size = 6\n",
    "max_epochs = 15\n",
    "hidden_size_linear = 64\n",
    "lr = 0.5\n",
    "batch_size = 128\n",
    "seq_len = max_len # Sequence length for RNN\n",
    "dropout_keep = 0.8\n",
    "input_size = num_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2c283af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape = (batch size, sequence length, num features) | N,L,Hin\n",
    "lstm_layer = nn.LSTM(num_feat, hidden_size, 5,batch_first=True,bidirectional = True)\n",
    "# output shape = (batch size, sequence length, 2 * hidden_size)\n",
    "\n",
    "# concatenate over num features and hidden size\n",
    "# output shape = (batch size, seq_len, 2 * hidden_size + num features)\n",
    "\n",
    "# linear layer\n",
    "linear_layer = nn.Linear(2 * hidden_size + num_feat,hidden_size_linear)\n",
    "# output size = (batch_size, seq_len, hidden_size_linear)\n",
    "\n",
    "# tanh - no change in dims\n",
    "tanh_layer = nn.Tanh()\n",
    "\n",
    "# dropout - no change of dimensions\n",
    "dropout_layer = nn.Dropout(dropout_keep)\n",
    "\n",
    "# fc layer - same operation as linear layer\n",
    "fc_layer = nn.Linear(hidden_size_linear,output_size)\n",
    "# output shape - (batch_size, hidden_size_linear)\n",
    "\n",
    "# softmax - no change\n",
    "softmax_layer = nn.Softmax(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4a376de6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input shape = (batch size, sequence length, num features) | N,L,Hin\n",
    "lstm_output,_ = lstm_layer(a_inp)\n",
    "# output shape = (batch size, sequence length, 2 * hidden_size)\n",
    "\n",
    "# concatenate over num features and hidden size\n",
    "input_features = torch.cat([lstm_output,a_inp],2)\n",
    "# output shape = (batch size, sequence length, 2 * hidden_size + num features)\n",
    "\n",
    "# linear output\n",
    "linear_output = linear_layer(input_features)\n",
    "# output size = (batch_size, seq_len, hidden_size_linear)\n",
    "\n",
    "linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
    "        \n",
    "max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
    "# max_out_features.shape = (batch_size, hidden_size_linear)\n",
    "\n",
    "max_out_features = dropout_layer(max_out_features)\n",
    "final_out = fc_layer(max_out_features)\n",
    "res = softmax_layer(final_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8b92b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RCNN, self).__init__()\n",
    "        # input shape = (batch size, sequence length, num features) | N,L,Hin\n",
    "        self.lstm_layer = nn.LSTM(num_feat, hidden_size, 5,batch_first=True,bidirectional = True)\n",
    "        # output shape = (batch size, sequence length, 2 * hidden_size)\n",
    "\n",
    "        # concatenate over num features and hidden size\n",
    "        # output shape = (batch size, seq_len, 2 * hidden_size + num features)\n",
    "\n",
    "        # linear layer\n",
    "        self.linear_layer = nn.Linear(2 * hidden_size + num_feat,hidden_size_linear)\n",
    "        # output size = (batch_size, seq_len, hidden_size_linear)\n",
    "\n",
    "        # tanh - no change in dims\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "\n",
    "        # dropout - no change of dimensions\n",
    "        self.dropout_layer = nn.Dropout(dropout_keep)\n",
    "\n",
    "        # fc layer - same operation as linear layer\n",
    "        self.fc_layer = nn.Linear(hidden_size_linear,output_size)\n",
    "        # output shape - (batch_size, hidden_size_linear)\n",
    "\n",
    "        # softmax - no change\n",
    "        self.softmax_layer = nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input shape = (batch size, sequence length, num features) | N,L,Hin\n",
    "        lstm_output,_ = self.lstm_layer(a_inp)\n",
    "        # output shape = (batch size, sequence length, 2 * hidden_size)\n",
    "\n",
    "        # concatenate over num features and hidden size\n",
    "        input_features = torch.cat([lstm_output,a_inp],2)\n",
    "        # output shape = (batch size, sequence length, 2 * hidden_size + num features)\n",
    "\n",
    "        # linear output\n",
    "        linear_output = self.linear_layer(input_features)\n",
    "        # output size = (batch_size, seq_len, hidden_size_linear)\n",
    "\n",
    "        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
    "\n",
    "        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
    "        # max_out_features.shape = (batch_size, hidden_size_linear)\n",
    "\n",
    "        max_out_features = self.dropout_layer(max_out_features)\n",
    "        final_out = self.fc_layer(max_out_features)\n",
    "        res = self.softmax_layer(final_out)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2f15f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "train_loss = 0\n",
    "\n",
    "# init\n",
    "model = RCNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# one iteration of forward prop\n",
    "optimizer.zero_grad()\n",
    "model_out = model(a_inp)\n",
    "fit = loss(model_out,a_out)\n",
    "\n",
    "# backprop\n",
    "fit.backward()\n",
    "train_loss += fit.item()\n",
    "optimizer.step()\n",
    "\n",
    "# test loss calculation\n",
    "with torch.no_grad():\n",
    "    predicted_out = model(a_inp)\n",
    "    fit = loss(mpredicted_outodel_out,a_out)\n",
    "    test_loss += fit.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebeb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
